{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3716,"status":"ok","timestamp":1756642432826,"user":{"displayName":"Hossein Saeidi","userId":"17076332236532865396"},"user_tz":-210},"id":"2sxbq1qqWwdJ","outputId":"faf9bf84-2c45-4046-8be4-ace1ed0f61c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Enabling notebook extension jupyter-js-widgets/extension...\n","      - Validating: \u001b[32mOK\u001b[0m\n"]}],"source":["try:\n","    from google.colab import drive\n","    from google.colab import output\n","    drive.mount('/content/drive')\n","    output.enable_custom_widget_manager()\n","    !jupyter nbextension enable --py widgetsnbextension\n","except:\n","    print(\"Not running in Google Colab, skipping drive mount and widget manager setup.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1756642432862,"user":{"displayName":"Hossein Saeidi","userId":"17076332236532865396"},"user_tz":-210},"id":"G4nTQ1FsS-bL","outputId":"eb000ff8-0e09-4fe3-d085-857e2eac97a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["All dependencies successfully imported!\n"]}],"source":["import os\n","import pickle\n","import random\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.optimize import curve_fit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Layer\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import tensorflow_probability as tfp\n","from tqdm import tqdm\n","import warnings\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import (Input, Dense, LayerNormalization,\n","                                   Dropout, GlobalAveragePooling1D, Conv1D,\n","                                   Embedding, Multiply, Concatenate, Lambda)\n","\n","\n","\n","print(\"All dependencies successfully imported!\")\n","\n","\n","tfd = tfp.distributions\n","# Configuration\n","warnings.filterwarnings('ignore')\n","plt.style.use('ggplot')\n","sns.set_style(\"whitegrid\")\n","pd.set_option('display.max_columns', 50)\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# Constants\n","SHIFT = 10800\n","FREQUENCY_START_MHZ = 0\n","FREQUENCY_END_MHZ = 10934 - SHIFT\n","BGS_RAW_TXT_PATH = '/content/drive/MyDrive/OstadSharif/FINAL/data/BGS_raw.txt'\n","BPS_RAW_TXT_PATH = '/content/drive/MyDrive/OstadSharif/FINAL/data/BPS_raw.txt'\n","\n","MODELS_DIR = '/content/drive/MyDrive/OstadSharif/FINAL/models'\n","LOG_DIR = '/content/drive/MyDrive/OstadSharif/FINAL/logs'\n","RESULTS_DIR = '/content/drive/MyDrive/OstadSharif/FINAL/results/PDNN_V3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_LxLnXyT_ol"},"outputs":[],"source":["def txt_to_numpy_array(file_path: str) -> np.ndarray:\n","    print(f\"Loading data from {file_path}...\")\n","    with open(file_path, 'r') as f:\n","        lines = f.readlines()\n","    data = np.array([list(map(float, line.strip().split(','))) for line in lines])\n","    data = data[:, ~np.isnan(data).any(axis=0)]  # Remove columns with NaNs\n","    print(f\"Loaded data shape: {data.shape}\")\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1756642433009,"user":{"displayName":"Hossein Saeidi","userId":"17076332236532865396"},"user_tz":-210},"id":"Ve8iPDxfJ6Xa","outputId":"817eedbc-4f9f-473a-b3f3-cbe606d525dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from /content/drive/MyDrive/OstadSharif/FINAL/data/BGS_raw.txt...\n","Loaded data shape: (68, 4500)\n","Loading data from /content/drive/MyDrive/OstadSharif/FINAL/data/BPS_raw.txt...\n","Loaded data shape: (68, 4501)\n"]}],"source":["real_bgs = txt_to_numpy_array(BGS_RAW_TXT_PATH)\n","real_bps = txt_to_numpy_array(BPS_RAW_TXT_PATH)[:,1:]\n","actual_num_freq_points, actual_num_distance_points = real_bgs.shape\n","distance_axis_idx = np.arange(actual_num_distance_points)\n","frequency_axis_mhz = np.linspace(FREQUENCY_START_MHZ, FREQUENCY_END_MHZ, actual_num_freq_points)\n","\n","\n","wvec = frequency_axis_mhz\n","xvec = distance_axis_idx\n","x0 = [40, 15]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kgc0mDBUDLJ"},"outputs":[],"source":["def bgs_scale(yvec, scaling='Standard'):\n","    if scaling == 'Standard':\n","        scaler = StandardScaler()\n","    elif scaling == 'MinMax':\n","        scaler = MinMaxScaler()\n","    elif scaling == 'Robust':\n","        scaler = RobustScaler()\n","    else:\n","        raise ValueError(\"Invalid scaling type\")\n","    return scaler.fit_transform(yvec.reshape(-1, 1)).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs6FLhacKYyO"},"outputs":[],"source":["def bps_scale(yvec, scaling='Standard'):\n","    if scaling is 'Standard':\n","        scaler = StandardScaler()\n","    elif scaling is 'MinMax':\n","        scaler = MinMaxScaler(feature_range=(-0.5,0.5))\n","    elif scaling is 'Robust':\n","        scaler = RobustScaler()\n","    else:\n","        raise ValueError()\n","    return scaler.fit_transform(yvec.reshape(len(yvec),1)).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvHnp-NuUTi0"},"outputs":[],"source":["def lorentzian_abs(x, g0, w, x0):\n","    return g0 * (w**2 / (4 * (x - x0)**2 + w**2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGBID-SHvNpK"},"outputs":[],"source":["def bgs_data_gen(wvec,bgs_parms,noise=0.0):\n","    n_w=len(wvec)\n","    bfs,fwhm = bgs_parms\n","    amp=1.0\n","    func=amp/(1+((wvec-bfs)/(fwhm/2))**2)\n","    func=func+noise*np.random.normal(0,1,n_w)\n","    return func"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btgZ7EjkKOZZ"},"outputs":[],"source":["def bps_data_gen(wvec,bps_parms,noise=0.0):\n","    n_w=len(wvec)\n","    bfs, fwhm = bps_parms\n","    amp=1.0\n","    func=amp*(wvec-bfs)*(fwhm/2)/((wvec-bfs)**2+(fwhm/2)**2)\n","    func=func+noise*np.random.normal(0,1,n_w)\n","    return func\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYcaJT76nbHm"},"outputs":[],"source":["def ls_fit(data,x0,mode='bgs'):\n","    xdata=data[:,0]\n","    ydataraw=data[:,1]\n","    if mode == 'bgs':\n","        ydata=bgs_scale(ydataraw,'MinMax')\n","        def fun(xdata,fpeak,fwhm):\n","            return bgs_data_gen(xdata,[fpeak,fwhm])\n","    elif mode == 'bps':\n","        ydata=bps_scale(ydataraw,'MinMax')\n","        def fun(xdata,fpeak,fwhm):\n","            return bps_data_gen(xdata,[fpeak,fwhm])\n","    else:\n","        raise ValueError()\n","    x,cov_x=curve_fit(fun, xdata,ydata, p0=x0,bounds=(np.array([0,5]), np.array([80,60])))\n","    x_err=[np.sqrt(cov_x[j,j]) for j in range(x.size)]\n","    return x,x_err"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enaQPAinnc6M"},"outputs":[],"source":["def batch_ls_fit(raw_data, wvec, x0, mode='bgs'):\n","    n_x = raw_data.shape[1]\n","    parmat = np.zeros((n_x, 2))\n","    parerr = np.zeros((n_x, 2))\n","    for i in tqdm(range(n_x)):\n","        data = np.column_stack([wvec, raw_data[:, i]])\n","        parmat[i, :], parerr[i, :] = ls_fit(data, x0, mode)\n","    return parmat, parerr\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb1B-5vaWmmy"},"outputs":[],"source":["wvec = frequency_axis_mhz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZQ7PXttpNiw"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, Input, Conv1D, Embedding\n","from tensorflow.keras.layers import GlobalAveragePooling1D, Flatten\n","from tensorflow.keras.models import Model\n","\n","# --------------------------\n","# Probabilistic Output Layer\n","# --------------------------\n","@tf.keras.utils.register_keras_serializable()\n","class ProbabilisticOutputLayer(Layer):\n","    def __init__(self, n_out=2, **kwargs):\n","        super().__init__(**kwargs)\n","        self.n_out = n_out\n","\n","    def call(self, inputs):\n","        if self.n_out == 2:\n","            loc = inputs[..., :2]\n","            scale = 1e-5 + tf.nn.softplus(0.1 * inputs[..., 2:])\n","            return tf.concat([loc, scale], axis=-1)\n","        elif self.n_out == 1:\n","            loc = inputs[..., :1]\n","            scale = 1e-5 + tf.nn.softplus(0.1 * inputs[..., 1:])\n","            return tf.concat([loc, scale], axis=-1)\n","        else:\n","            raise ValueError(\"Invalid number of outputs\")\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({'n_out': self.n_out})\n","        return config\n","\n","\n","# --------------------------\n","# Transformer Block\n","# --------------------------\n","@tf.keras.utils.register_keras_serializable()\n","class TransformerBlock(Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential([\n","            Dense(ff_dim, activation='relu'),\n","            Dense(embed_dim),\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training=False):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","# --------------------------\n","# Frequency-Aware Embedding\n","# --------------------------\n","@tf.keras.utils.register_keras_serializable()\n","class FrequencyAwareEmbedding(Layer):\n","    def __init__(self, max_freq, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_freq = max_freq\n","        self.embed_dim = embed_dim\n","        self.freq_embedding = Embedding(input_dim=max_freq+1, output_dim=embed_dim)\n","\n","    def call(self, inputs):\n","        freq_indices = tf.cast(inputs[:, :, 0], tf.int32)\n","        amplitude_values = inputs[:, :, 1]\n","        freq_embeddings = self.freq_embedding(freq_indices)\n","        amplitude_values = tf.expand_dims(amplitude_values, axis=-1)\n","        return freq_embeddings * amplitude_values\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            'max_freq': self.max_freq,\n","            'embed_dim': self.embed_dim\n","        })\n","        return config\n","\n","\n","# --------------------------\n","# Add Noise Layer\n","# --------------------------\n","@tf.keras.utils.register_keras_serializable()\n","class AddNoise(Layer):\n","    def __init__(self, stddev=0.01, **kwargs):\n","        super().__init__(**kwargs)\n","        self.stddev = stddev\n","\n","    def call(self, inputs, training=False):\n","        if training:\n","            noise = tf.random.normal(tf.shape(inputs), mean=0.0, stddev=self.stddev)\n","            return inputs + noise\n","        return inputs\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({'stddev': self.stddev})\n","        return config\n","\n","\n","# --------------------------\n","# Attention Pooling Layer\n","# --------------------------\n","@tf.keras.utils.register_keras_serializable()\n","class AttentionPooling(Layer):\n","    def __init__(self, units, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dense = Dense(units, activation='tanh')\n","        self.context_vector = Dense(1, use_bias=False)\n","\n","    def call(self, inputs):\n","        score = self.context_vector(self.dense(inputs))   # (batch, seq_len, 1)\n","        weights = tf.nn.softmax(score, axis=1)\n","        context = tf.reduce_sum(weights * inputs, axis=1)\n","        return context\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4D9Xr4WpxxD"},"outputs":[],"source":["class LRTracker(keras.callbacks.Callback):\n","    def __init__(self):\n","        super().__init__()\n","        self._lrs = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        current_lr = keras.backend.get_value(self.model.optimizer.learning_rate)\n","        self._lrs.append(current_lr)\n","        logs['lr'] = current_lr\n","\n","@tf.keras.utils.register_keras_serializable()\n","def negloglik(y_true, y_pred):\n","    if y_pred.shape[-1] == 4:  # n_out == 2\n","        loc = y_pred[..., :2]  # BFS and FWHM means\n","        scale = y_pred[..., 2:]  # BFS and FWHM stds\n","        distribution = tfd.Independent(\n","            tfd.Normal(loc=loc, scale=scale),\n","            reinterpreted_batch_ndims=1\n","        )\n","    elif y_pred.shape[-1] == 2:  # n_out == 1\n","        loc = y_pred[..., :1]\n","        scale = y_pred[..., 1:]\n","        distribution = tfd.Normal(loc=loc, scale=scale)\n","    else:\n","        raise ValueError(\"Invalid output shape\")\n","    return -distribution.log_prob(y_true)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgB31diaUipx"},"outputs":[],"source":["class PDNN:\n","    def __init__(self, wvec=None, channels=2, scaler='Standard', mode='bgs', batch_size=1000, max_epochs=500):\n","        self.mode = mode\n","        self.model = None\n","        self.model_history = 'Network has not been trained'\n","        self.wvec = wvec\n","        self.input_shape = (len(wvec), channels)\n","\n","        if self.mode == 'bgs':\n","          self.n_out = 2\n","\n","        elif self.mode == 'bps':\n","          self.n_out = 1\n","        else:\n","          print(\"error mode\")\n","        self.scaling = scaler\n","        self.batch_size = batch_size\n","        self.max_epochs = max_epochs\n","        self.model_path = os.path.join(MODELS_DIR, f'{self.mode}_pdnn_v3.keras')\n","        self.log_dir = LOG_DIR\n","        self.results_dir = RESULTS_DIR\n","        self.lr_tracker = LRTracker()\n","\n","    def data_batchgen(self, parmat):\n","        n_w, n_c = self.input_shape\n","        N = parmat.shape[0]\n","        func_parms = parmat[:, 0:self.n_out]\n","        func_results = np.zeros((N, n_w, n_c))\n","        for i in tqdm(range(N), \"generating data...\"):\n","            func_results[i, :, 0] = self.wvec\n","            if self.mode == 'bgs':\n","                bgsvec = bgs_data_gen(self.wvec, parmat[i, :2], noise=parmat[i, 2])\n","                func_results[i, :, 1] = bgs_scale(bgsvec, scaling=self.scaling)\n","            elif self.mode == 'bps':\n","                bpsvec = bps_data_gen(self.wvec, parmat[i, :2], noise=parmat[i, 2])\n","                func_results[i, :, 1] = bps_scale(bpsvec, scaling=self.scaling)\n","            else:\n","                raise ValueError(\"Invalid mode\")\n","        return func_results, func_parms\n","\n","    def create_model(self, max_freq=134):\n","        inputs = Input(shape=self.input_shape)\n","\n","        # Frequency-aware embedding\n","        x = FrequencyAwareEmbedding(max_freq, 128)(inputs)\n","        x = AddNoise(stddev=0.01)(x)\n","\n","        # Local feature extraction with CNN\n","        x = Conv1D(128, kernel_size=7, padding='same', activation='relu')(x)\n","        x = Conv1D(128, kernel_size=5, padding='same', activation='relu')(x)\n","\n","        # Stacked Transformers\n","        x = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=256)(x)\n","        x = TransformerBlock(embed_dim=128, num_heads=8, ff_dim=512)(x)\n","        x = TransformerBlock(embed_dim=128, num_heads=8, ff_dim=512)(x)\n","\n","        # Attention pooling\n","        x = AttentionPooling(units=128)(x)\n","\n","        # Dense head\n","        x = Dense(256, activation='relu')(x)\n","        x = Dropout(0.3)(x)\n","        x = Dense(2 * self.n_out, activation='linear')(x)\n","\n","        outputs = ProbabilisticOutputLayer(n_out=self.n_out)(x)\n","\n","        self.model = Model(inputs=inputs, outputs=outputs, name=\"SpectralModel_V3\")\n","\n","\n","    def compile_model(self, initial_lr=0.005):\n","        print(\"Compiling model with negative log-likelihood loss...\")\n","        self.model.compile(\n","            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n","            loss=negloglik\n","        )\n","        self.callbacks = [\n","            EarlyStopping(\n","                monitor='val_loss',\n","                min_delta=0.01,\n","                patience=32,\n","                verbose=1,\n","                restore_best_weights=True,\n","            ),\n","            ReduceLROnPlateau(\n","                monitor='val_loss',\n","                factor=0.5,\n","                patience=8,\n","                verbose=0,\n","                mode='auto',\n","                min_delta=1e-3,\n","                min_lr=1e-6\n","            ),\n","            ModelCheckpoint(\n","                self.model_path,\n","                monitor='val_loss',\n","                save_best_only=True,\n","                verbose=1\n","            ),\n","            self.lr_tracker\n","        ]\n","\n","    def train_and_visualize(self, X, y, epochs=None, batch_size=None):\n","        print(\"Starting model training...\")\n","        epochs = epochs or self.max_epochs\n","        batch_size = batch_size or self.batch_size\n","        self.history = self.model.fit(\n","            X, y,\n","            validation_split=0.3,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            callbacks=self.callbacks,\n","            verbose=2,\n","            shuffle=True\n","        )\n","        plt.figure(figsize=(18, 12))\n","        sns.set_style(\"whitegrid\")\n","        palette = sns.color_palette(\"husl\", 8)\n","        plt.subplot(2, 2, 1)\n","        plt.plot(self.history.history['loss'], label='Train', color=palette[0])\n","        plt.plot(self.history.history['val_loss'], label='Validation', color=palette[1])\n","        plt.title('Model Loss')\n","        plt.ylabel('Loss')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.subplot(2, 2, 4)\n","        if hasattr(self.lr_tracker, '_lrs'):\n","            plt.plot(self.lr_tracker._lrs, label='Learning Rate', color=palette[6])\n","            plt.title('Learning Rate Schedule')\n","            plt.ylabel('LR')\n","            plt.xlabel('Epoch')\n","            plt.yscale('log')\n","            plt.legend()\n","        else:\n","            plt.text(0.5, 0.5, 'Learning Rate Not Tracked', ha='center', va='center')\n","            plt.title('Learning Rate Schedule (Not Available)')\n","            plt.axis('off')\n","        plt.tight_layout()\n","        plt.show()\n","        print(\"Training completed.\")\n","\n","    def full_pipeline(self, N=300000, minvals=(10.0, 10.0, 0.0), maxvals=(130.0, 60.0, 0.05)):\n","        print(\"Starting full pipeline...\")\n","        print(\"Generating synthetic data...\")\n","        parmat = np.random.uniform(minvals, maxvals, (N, 3))\n","        X, y = self.data_batchgen(parmat)\n","        self.create_model()\n","        self.compile_model()\n","        print(\"Fitting the model...\")\n","        self.train_and_visualize(X, y, epochs=512, batch_size=1024)\n","        print(\"Pipeline completed.\")\n","\n","    def batch_predict(self, data):\n","        n_w, n_c = self.input_shape\n","        N = data.shape[1]\n","        inputs = np.zeros((N, n_w, n_c))\n","        for i in range(N):\n","            inputs[i, :, 0] = self.wvec\n","            if self.mode == 'bgs':\n","                inputs[i, :, 1] = bgs_scale(data[:, i], scaling=self.scaling)\n","            elif self.mode == 'bps':\n","                inputs[i, :, 1] = bps_scale(data[:, i], scaling=self.scaling)\n","            else:\n","                raise ValueError(\"Invalid mode\")\n","\n","        predictions = self.model.predict(inputs)\n","        if self.n_out == 2:\n","            par_mean = predictions[:, :2]  # BFS mean, FWHM mean\n","            par_std = predictions[:, 2:]   # BFS std, FWHM std\n","        else:\n","            par_mean = predictions[:, :1]  # BFS mean\n","            par_std = predictions[:, 1:]   # BFS std\n","        return par_mean, par_std\n","\n","    def file_dnn_predict(self, file):\n","        raw_data = txt_to_numpy_array(file)\n","        par_mean, par_std = self.batch_predict(raw_data)\n","        return par_mean, par_std\n","\n","    def load_model(self):\n","        if self.model is None:\n","            self.model = tf.keras.models.load_model(\n","                self.model_path,\n","                custom_objects={\n","                    'TransformerBlock': TransformerBlock,\n","                    'AttentionPooling': AttentionPooling,\n","                    'ProbabilisticOutputLayer': ProbabilisticOutputLayer,\n","                    'negloglik': negloglik\n","                }\n","            )\n","        return self.model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvF7a9qdnp0f","outputId":"51da48e5-15ef-4ea4-924c-4c81549f4243"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting full pipeline...\n","Generating synthetic data...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["generating data...: 100%|██████████| 300000/300000 [02:39<00:00, 1876.94it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Compiling model with negative log-likelihood loss...\n","Fitting the model...\n","Starting model training...\n","Epoch 1/512\n","\n","Epoch 1: val_loss improved from inf to 5.65170, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 168s - 817ms/step - loss: 75.7349 - val_loss: 5.6517 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 2/512\n","\n","Epoch 2: val_loss improved from 5.65170 to 4.98740, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 170s - 825ms/step - loss: 5.4460 - val_loss: 4.9874 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 3/512\n","\n","Epoch 3: val_loss improved from 4.98740 to 4.97128, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 142s - 689ms/step - loss: 5.0558 - val_loss: 4.9713 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 4/512\n","\n","Epoch 4: val_loss did not improve from 4.97128\n","206/206 - 142s - 687ms/step - loss: 5.0426 - val_loss: 4.9733 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 5/512\n","\n","Epoch 5: val_loss did not improve from 4.97128\n","206/206 - 142s - 689ms/step - loss: 5.0324 - val_loss: 4.9745 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 6/512\n","\n","Epoch 6: val_loss did not improve from 4.97128\n","206/206 - 142s - 690ms/step - loss: 5.0214 - val_loss: 4.9796 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 7/512\n","\n","Epoch 7: val_loss improved from 4.97128 to 4.97117, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 142s - 690ms/step - loss: 5.0091 - val_loss: 4.9712 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 8/512\n","\n","Epoch 8: val_loss improved from 4.97117 to 4.97042, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 128s - 622ms/step - loss: 5.0035 - val_loss: 4.9704 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 9/512\n","\n","Epoch 9: val_loss did not improve from 4.97042\n","206/206 - 142s - 687ms/step - loss: 5.0013 - val_loss: 4.9729 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 10/512\n","\n","Epoch 10: val_loss improved from 4.97042 to 4.96903, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 142s - 690ms/step - loss: 4.9937 - val_loss: 4.9690 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 11/512\n","\n","Epoch 11: val_loss improved from 4.96903 to 4.96900, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 142s - 689ms/step - loss: 4.9919 - val_loss: 4.9690 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 12/512\n","\n","Epoch 12: val_loss improved from 4.96900 to 4.96900, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 128s - 621ms/step - loss: 4.9892 - val_loss: 4.9690 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 13/512\n","\n","Epoch 13: val_loss improved from 4.96900 to 4.96785, saving model to /content/drive/MyDrive/OstadSharif/FINAL/models/bps_pdnn_v3.keras\n","206/206 - 142s - 690ms/step - loss: 4.9875 - val_loss: 4.9678 - learning_rate: 0.0050 - lr: 0.0050\n","Epoch 14/512\n"]}],"source":["pdnn = PDNN(wvec=frequency_axis_mhz, mode='bps')\n","# pdnn.load_model()\n","pdnn.full_pipeline()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_Cjhff5bX7z"},"outputs":[],"source":["def BOTDA_plotfn(xvec, parmean, parstd, filename, mode='bgs',\n","                 figsize=(12,3), figfmt='svg', ylims=None):\n","    if mode == 'bgs':\n","        ylabels = ['BFS (MHz)', 'FWHM (MHz)']\n","        fig, ax = plt.subplots(1, 2, figsize=figsize)\n","        # Plot for BFS (first subplot)\n","        ax[0].plot(xvec, parmean[:, 0], 'k-', label='predicted mean')\n","        ax[0].set_xlabel('Fiber Length (km)')\n","        ax[0].set_ylabel(ylabels[0])\n","        ymin = parmean[:, 0] - 3 * parstd[:, 0]\n","        ymax = parmean[:, 0] + 3 * parstd[:, 0]\n","        ax[0].fill_between(xvec, ymin.flatten(), ymax.flatten(), color='blue', alpha=0.25, label='99% confidence interval')\n","        ax[0].legend(loc='lower right')\n","        ax[0].set_title(r\"$\\sigma = %5.3f$\" % (np.std(parmean[:, 0])))\n","        ax[0].grid()\n","        if ylims is None:\n","            ax[0].set_ylim([0.98 * np.min(ymin), 1.02 * np.max(ymax)])\n","        else:\n","            ax[0].set_ylim(ylims[0])\n","\n","        # Plot for FWHM (second subplot)\n","        ax[1].plot(xvec, parmean[:, 1], 'k-', label='predicted mean')\n","        ax[1].set_xlabel('Fiber Length (km)')\n","        ax[1].set_ylabel(ylabels[1])\n","        ymin = parmean[:, 1] - 3 * parstd[:, 1]\n","        ymax = parmean[:, 1] + 3 * parstd[:, 1]\n","        ax[1].fill_between(xvec, ymin.flatten(), ymax.flatten(), color='blue', alpha=0.25, label='99% confidence interval')\n","        ax[1].legend(loc='lower right')\n","        ax[1].set_title(r\"$\\sigma = %5.3f$\" % (np.std(parmean[:, 1])))\n","        ax[1].grid()\n","        if ylims is None:\n","            ax[1].set_ylim([0.98 * np.min(ymin), 1.02 * np.max(ymax)])\n","        else:\n","            ax[1].set_ylim(ylims[1])\n","\n","        plt.tight_layout(w_pad=2.0)\n","\n","    elif mode == 'bps':\n","        ylabels = ['BFS (MHz)']\n","        fig, ax = plt.subplots(1, 1, figsize=figsize)\n","        ax.plot(xvec, parmean[:, 0], 'k-', label='predicted mean')\n","        ax.set_xlabel('Fiber Length (km)')\n","        ax.set_ylabel(ylabels[0])\n","        ymin = parmean[:, 0] - 3 * parstd[:, 0]\n","        ymax = parmean[:, 0] + 3 * parstd[:, 0]\n","        ax.fill_between(xvec, ymin.flatten(), ymax.flatten(), color='blue', alpha=0.25, label='99% confidence interval')\n","        ax.legend(loc='lower right')\n","        ax.set_title(r\"$\\sigma = %5.3f$\" % (np.std(parmean[:, 0])))\n","        ax.grid()\n","        if ylims is None:\n","            ax.set_ylim([0.98 * np.min(ymin), 1.02 * np.max(ymax)])\n","        else:\n","            ax.set_ylim(ylims[0])\n","\n","        plt.tight_layout(w_pad=2.0)\n","\n","    plt.savefig(filename, format=figfmt, dpi=600)\n","\n","def evaluate_botda_methods(raw_data, pdnn, wvec, x0, mode, xvec, plot_dir='.', figfmt='svg', ylims=None):\n","    \"\"\"\n","    Function to evaluate curve fitting and PDNN methods for BOTDA data.\n","\n","    Inputs:\n","    - raw_data: Input data for fitting and prediction.\n","    - pdnn: Deep learning model object with batch_predict method.\n","    - wvec: Frequency vector for least squares fitting.\n","    - x0: Initial guess for least squares fitting.\n","    - mode: 'bps' for BFS only, 'bgs' for BFS and FWHM.\n","    - xvec: Fiber length vector for plotting.\n","    - plot_dir: Directory to save plots.\n","    - figfmt: Format for saving figures.\n","    - ylims: Optional y-limits for plots.\n","\n","    Outputs:\n","    - Returns a dictionary with computed variables, times, statistical evaluations, and comparisons.\n","    - Saves plots for LS, NN, and differences.\n","    \"\"\"\n","    # Least squares curve fitting\n","    start = time.time()\n","    LS_parmat, LS_err = batch_ls_fit(raw_data, wvec, x0, mode=mode)\n","    curve_fitting_time = time.time() - start\n","\n","    # PDNN prediction\n","    start = time.time()\n","    NN_par_mean, NN_par_std = pdnn.batch_predict(raw_data)\n","    PDNN_time = time.time() - start\n","\n","    # Prepare results dictionary\n","    results = {\n","        'curve_fitting_time': curve_fitting_time,\n","        'PDNN_time': PDNN_time,\n","        'LS_par_mean': LS_parmat,\n","        'LS_par_std': LS_err,\n","        'NN_par_mean': NN_par_mean,\n","        'NN_par_std': NN_par_std\n","    }\n","\n","    # Statistical evaluations and comparisons\n","    if mode == 'bps':\n","        bfs_ls = LS_parmat[:, 0]\n","        bfs_ls_std = LS_err[:, 0]\n","        bfs_nn = NN_par_mean[:, 0]\n","        bfs_nn_std = NN_par_std[:, 0]\n","\n","        # Differences and errors for BFS\n","        diff_bfs = bfs_nn - bfs_ls\n","        mae_bfs = np.mean(np.abs(diff_bfs))\n","        rmse_bfs = np.sqrt(np.mean(diff_bfs**2))\n","        mean_diff_bfs = np.mean(diff_bfs)\n","        std_diff_bfs = np.std(diff_bfs)\n","\n","        results.update({\n","            'mae_bfs': mae_bfs,\n","            'rmse_bfs': rmse_bfs,\n","            'mean_diff_bfs': mean_diff_bfs,\n","            'std_diff_bfs': std_diff_bfs\n","        })\n","\n","        # Plots\n","        BOTDA_plotfn(xvec, LS_parmat, LS_err, f'{plot_dir}/ls_{mode}.{figfmt}', mode=mode, ylims=ylims)\n","        BOTDA_plotfn(xvec, NN_par_mean, NN_par_std, f'{plot_dir}/nn_{mode}.{figfmt}', mode=mode, ylims=ylims)\n","\n","        # Difference plot for BFS\n","        fig, ax = plt.subplots(figsize=(12, 3))\n","        ax.plot(xvec, diff_bfs, 'r-', label='NN - LS')\n","        ax.set_xlabel('Fiber Length (km)')\n","        ax.set_ylabel('Difference BFS (MHz)')\n","        ax.set_title(f'BFS Difference (MAE: {mae_bfs:.3f}, RMSE: {rmse_bfs:.3f})')\n","        ax.legend()\n","        ax.grid()\n","        plt.tight_layout()\n","        plt.savefig(f'{plot_dir}/diff_bfs_{mode}.{figfmt}', format=figfmt, dpi=600)\n","\n","    elif mode == 'bgs':\n","        bfs_ls = LS_parmat[:, 0]\n","        fwhm_ls = LS_parmat[:, 1]\n","        bfs_ls_std = LS_err[:, 0]\n","        fwhm_ls_std = LS_err[:, 1]\n","        bfs_nn = NN_par_mean[:, 0]\n","        fwhm_nn = NN_par_mean[:, 1]\n","        bfs_nn_std = NN_par_std[:, 0]\n","        fwhm_nn_std = NN_par_std[:, 1]\n","\n","        # Differences and errors for BFS\n","        diff_bfs = bfs_nn - bfs_ls\n","        mae_bfs = np.mean(np.abs(diff_bfs))\n","        rmse_bfs = np.sqrt(np.mean(diff_bfs**2))\n","        mean_diff_bfs = np.mean(diff_bfs)\n","        std_diff_bfs = np.std(diff_bfs)\n","\n","        # Differences and errors for FWHM\n","        diff_fwhm = fwhm_nn - fwhm_ls\n","        mae_fwhm = np.mean(np.abs(diff_fwhm))\n","        rmse_fwhm = np.sqrt(np.mean(diff_fwhm**2))\n","        mean_diff_fwhm = np.mean(diff_fwhm)\n","        std_diff_fwhm = np.std(diff_fwhm)\n","\n","        results.update({\n","            'mae_bfs': mae_bfs,\n","            'rmse_bfs': rmse_bfs,\n","            'mean_diff_bfs': mean_diff_bfs,\n","            'std_diff_bfs': std_diff_bfs,\n","            'mae_fwhm': mae_fwhm,\n","            'rmse_fwhm': rmse_fwhm,\n","            'mean_diff_fwhm': mean_diff_fwhm,\n","            'std_diff_fwhm': std_diff_fwhm\n","        })\n","\n","        # Plots\n","        BOTDA_plotfn(xvec, LS_parmat, LS_err, f'{plot_dir}/ls_{mode}.{figfmt}', mode=mode, ylims=ylims)\n","        BOTDA_plotfn(xvec, NN_par_mean, NN_par_std, f'{plot_dir}/nn_{mode}.{figfmt}', mode=mode, ylims=ylims)\n","\n","        # Difference plot for BFS and FWHM\n","        fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n","        ax[0].plot(xvec, diff_bfs, 'r-', label='NN - LS')\n","        ax[0].set_xlabel('Fiber Length (km)')\n","        ax[0].set_ylabel('Difference BFS (MHz)')\n","        ax[0].set_title(f'BFS Difference (MAE: {mae_bfs:.3f}, RMSE: {rmse_bfs:.3f})')\n","        ax[0].legend()\n","        ax[0].grid()\n","\n","        ax[1].plot(xvec, diff_fwhm, 'r-', label='NN - LS')\n","        ax[1].set_xlabel('Fiber Length (km)')\n","        ax[1].set_ylabel('Difference FWHM (MHz)')\n","        ax[1].set_title(f'FWHM Difference (MAE: {mae_fwhm:.3f}, RMSE: {rmse_fwhm:.3f})')\n","        ax[1].legend()\n","        ax[1].grid()\n","\n","        plt.tight_layout(w_pad=2.0)\n","        plt.savefig(f'{plot_dir}/diff_{mode}.{figfmt}', format=figfmt, dpi=600)\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NreyHfRrblJn"},"outputs":[],"source":["bps_results = evaluate_botda_methods(real_bps, pdnn, wvec=wvec, x0=x0, mode='bps', xvec=xvec, plot_dir=RESULTS_DIR, figfmt='svg', ylims=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vjv1Zahc_6v"},"outputs":[],"source":["pdnn_bgs = PDNN(wvec=frequency_axis_mhz, mode='bgs')\n","# pdnn_bgs.load_model()\n","pdnn_bgs.full_pipeline()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_N6ueH5pdCOo"},"outputs":[],"source":["bgs_results = evaluate_botda_methods(real_bgs, pdnn_bgs, wvec=wvec, x0=x0, mode='bgs', xvec=xvec, plot_dir=RESULTS_DIR, figfmt='svg', ylims=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6hkdO5W7f6R"},"outputs":[],"source":["import pickle\n","res = {\n","    'bps':bps_results,\n","    'bgs':bgs_results\n","}\n","with open(f\"{RESULTS_DIR}STATS.pickel\", 'wb') as f:\n","  pickle.dump(res, f)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}